---
title: "CycleBNN: Cyclic Precision Training in Binary Neural Networks"
authors: "Fontana, F., <b>Lanzino, R.</b>, Diko, A., Foresti, G. L., & Cinque, L."
collection: publications
category: conferences
permalink: /publication/2024_eccvw_2
# excerpt: none
date: 2024-09-01
venue: 'European Conference on Computer Vision Workshops (ECCVW)'
# slidesurl: 'http://academicpages.github.io/files/slides1.pdf'
# paperurl: 'https://diglib.eg.org/bitstreams/b60aa1b9-6b9d-4ebd-b3d4-120d5ed1058c/download'
# bibtexurl: 'http://academicpages.github.io/files/bibtex1.bib'
citation: 'Fontana, F., Lanzino, R., Diko, A., Foresti, G. L., & Cinque, L. (2025). CycleBNN: Cyclic Precision Training in Binary Neural Networks. In A. Del Bue, C. Canton, J. Pont-Tuset, & T. Tommasi (Eds.), Computer Vision -- ECCV 2024 Workshops (pp. 113–130). Cham: Springer Nature Switzerland.'
---
This paper works on Binary Neural Networks (BNNs), a promising avenue for efficient deep learning, offering significant reductions in computational overhead and memory footprint to full precision networks. However, the challenge of energy-intensive training and the drop in performance have been persistent issues. Tackling the challenge, prior works focus primarily on task-related inference optimization. Unlike prior works, this study offers an innovative methodology integrating BNNs with cyclic precision training, introducing the CycleBNN. This approach is designed to enhance training efficiency while minimizing the loss in performance. By dynamically adjusting precision in cycles, we achieve a convenient trade-off between training efficiency and model performance. This emphasizes the potential of our method in energy-constrained training scenarios, where data is collected onboard and paves the way for sustainable and efficient deep learning architectures. To gather insights on CycleBNN’s efficiency, we conduct experiments on ImageNet, CIFAR-10, and PASCAL-VOC, obtaining competitive performances while using 96.09% less operations during training on ImageNet, 88.88% on CIFAR-10 and 96.09% on PASCAL-VOC. Finally, CycleBNN offers a path towards faster, more accessible training of efficient networks, accelerating the development of practical applications. The PyTorch code is available at https://github.com/fedeloper/CycleBNN/.